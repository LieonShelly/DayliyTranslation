[åŸæ–‡](https://medium.com/flawless-app-stories/detecting-avengers-superheroes-in-your-ios-app-with-ibm-watson-and-coreml-fe38e493a4d1)

# Machine Learning in iOS: IBM Watson andÂ CoreML
Part 1

# iOSæœºæ¢°å­¦ä¹ ï¼šIBM Watson ä¸Â CoreML(ç¬¬ä¸€éƒ¨åˆ†)

Apple introducedÂ CoreMLÂ in WWDC 2017, and it is a great deal.Â 
CoreML is a machine learning framework used in many Apple products, like Siri, Camera, Keyboard Dictation, etc.

 Appleåœ¨2017å¹´çš„WWDCæ¨å‡ºäº†CoreMLæ¡†æ¶ï¼Œè¿™æ— ç–‘æ˜¯ä¸ªä¼Ÿå¤§çš„å†³ç­–ï¼ŒCoreMLæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œè¢«å¤§é‡çš„ç”¨äºè‹¹æœç›¸å…³çš„äº§å“ï¼Œæ¯”å¦‚è‹¹æœSiriï¼Œç›¸æœºï¼Œé”®ç›˜å¬å†™ç­‰ç­‰ã€‚
 
 Itâ€™s the foundation for Vision and Natural language processing. The cool stuff about CoreML is that it can use a pre-trained model to work offline. Apple has provided lots of pre-trainedÂ modelsÂ like MobileNet, SqueezeNet, Inception v3, VGG16 to help us with image recognition tasks, especially detecting dominant objects in a scene.
 
CoreMLæ˜¯Visionå’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€æ¡†æ¶ã€‚CoreMLçš„ä¼˜åŠ¿æ˜¯å®ƒå¯ä»¥ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹æ¥ç¦»çº¿å·¥ä½œã€‚è‹¹æœæä¾›äº†å¤§é‡çš„é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ¯”å¦‚MobileNetSqueezeNet, Inception v3, VGG16ç­‰ç­‰ï¼Œæ¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œå›¾åƒè¯†åˆ«ã€‚æä¾›çš„æ¨¡å‹ä¸»è¦æ˜¯åœ¨æœåŠ¡äºæ£€æµ‹åœºæ™¯ã€‚
 
The job of CoreML is simply predicting data based on the models. If the provided trained models do not suit our needs, we can train the model with our own dataset. There are frameworks likeÂ Keras,Â TensorFlow,Â CaffeÂ or the simplifiedÂ turicreate, but those requires understanding of machine learning and computer power to train the models. An easy (and costly approach) is to useÂ cloudÂ services, likeÂ Google Cloud Vision,Â IBM Watson,Â Microsoft Azure Cognitive Service,Â Amazon Rekognition,Â â€¦ to simplify the tasks. These services usually host the full models, and retrain them, so we just need to make HTTP requests to get information about classified objects. Some offer the ability to extend the models by allowing us to upload our data set and define classifiers.

CoreMLæ‰€åšçš„å·¥ä½œä»…ä»…æ˜¯åŸºäºæ¨¡å‹æ¥é¢„æµ‹æ•°æ®ã€‚å¦‚æœæ‰€æä¾›çš„è®­ç»ƒæ¨¡å‹ä¸æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œæˆ‘ä»¬åªéœ€è¦é€šè¿‡æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚æœ‰åƒKerasï¼ŒTensorFlowï¼ŒCaffeæˆ–ç®€åŒ–çš„turicsreateè¿™æ ·çš„æ¡†æ¶ï¼Œä½†é‚£äº›éœ€è¦äº†è§£æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºèƒ½åŠ›æ¥è®­ç»ƒæ¨¡å‹ã€‚ä½†æœ‰ç›¸å¯¹ç®€å•çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨äº‘æœåŠ¡æ¥ç®€åŒ–è¿™äº›å·¥ä½œï¼Œæ¯”å¦‚Â Google Cloud Vision,Â IBM Watson,Â Microsoft Azure Cognitive Service,Â Amazon Rekognitionã€‚ è¿™äº›æœåŠ¡é€šå¸¸æ‰˜ç®¡äº†æ•´ä¸ªæ¨¡å‹ï¼Œå¹¶é‡æ–°è®­ç»ƒäº†å®ƒä»¬ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å‘é€HTTPè¯·æ±‚å»è·å–ç›¸å…³åˆ†ç±»å¥½çš„æ¨¡å‹ã€‚ æœ‰äº›æä¾›äº†æ‰©å±•æ¨¡å‹çš„åŠŸèƒ½ï¼Œå…è®¸æˆ‘ä»¬åˆ é™¤æ•°æ®å¹¶å®šä¹‰åˆ†ç±»å™¨

In this guide, we will explore IBM Watson Service, especially its Visual Recognition feature to help us train a custom dataset of superheroes images, then consume the trained model in our iOS app via CoreML.

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢IBM Watson Serviceï¼Œç‰¹åˆ«æ˜¯å…¶è§†è§‰è¯†åˆ«åŠŸèƒ½ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬è®­ç»ƒè¶…çº§è‹±é›„å›¾åƒçš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œç„¶åé€šè¿‡COreMLåœ¨æˆ‘ä»¬çš„iOSåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ç»è¿‡è®­ç»ƒå¥½çš„æ¨¡å‹

# IBM WatsonÂ Service

Just recently Apple has announced partnership withÂ IBM WatsonÂ that makes it easier to get started with machine learning.

å°±åœ¨æœ€è¿‘ï¼ŒAppleå®£å¸ƒä¸IBM Watsonåˆä½œï¼Œè¿™ä½¿å¾—æœºæ¢°å­¦ä¹ å¼€å§‹å˜å¾—æ›´åŠ å®¹æ˜“

>>You can build apps that leverage Watson models on iPhone and iPad, 
>>even when offline. Your apps can quickly analyze images, accurately 
>>classify visual content, and easily train models using Watson Services

>>å³ä½¿åœ¨ç¦»çº¿çš„æƒ…å†µä¸‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥å†iPhoneå’ŒiPAdä¸Šæ„å»ºWatsonæ¨¡å‹çš„åº”ç”¨ç¨‹åºï¼Œæ‚¨çš„åº”ç”¨ç¨‹
>>åºå¯ä»¥å¿«é€Ÿçš„åˆ†æå›¾åƒï¼Œå‡†ç¡®åˆ†ç±»è§†è§‰å†…å®¹ï¼Œå¹¶ä½¿ç”¨Watson Servicesè½»æ¾è®­ç»ƒæ¨¡å‹

So letâ€™s try it first to see how it goes. The filmÂ Avengers Infinity WarÂ is popular on the cinema now and the superheroes still bumping around our heads. Maybe some of your friends wonâ€™t recognise those lots of superheroes in the movie. So letâ€™s have a fun time by building an app that can help recognise superheroes.

æ‰€ä»¥é¦–å…ˆçœ‹ä¸‹æˆ‘ä»¬çš„åŠŸèƒ½æ˜¯ä¸ªä»€ä¹ˆæ ·çš„ã€‚è¿‘æ¥ï¼Œå¤ä»‡è€…è”ç›Ÿè¿™éƒ¨ç”µå½±åœ¨é™¢çº¿éå¸¸çš„ç«çƒ­ï¼Œè¶…çº§è‹±é›„ä»¬åœ¨è„‘æµ·ä¸­å›è¡ã€‚ä½†æ˜¯ä½ çš„æœ‹å‹ä»¬å¯èƒ½å¹¶ä¸èƒ½å®Œå…¨è®°å¾—ç”µå½±ä¸­çš„å¤§é‡çš„è¶…çº§è‹±é›„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªåº”ç”¨æ¥å¸®åŠ©ä»–ä»¬è¯†åˆ«è¿™äº›è¶…çº§è‹±é›„ä»¬ã€‚

Playing with cloud services is straightforward. We pay some money for the service, upload superheroes pictures, train on the cloud, get the trained model to the app and enjoy the magic ğŸ˜.

ç©äº‘æœåŠ¡å¾ˆç®€å•ã€‚æˆ‘ä»¬ä»˜ä¸€äº›é’±ç»™äº‘æœåŠ¡ï¼Œä¸Šä¼ è¶…çº§è‹±é›„çš„å›¾ç‰‡ï¼Œåœ¨äº‘æœåŠ¡å™¨ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä»appä¸­è·å–è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæœ€åè§è¯äº‘æœåŠ¡å¸¦æ¥çš„ç¥å™¨æ•ˆæœ

Iâ€™ve been reading someÂ articlesÂ and trying IBM Cloud but somehow I still find it confusing. Mostly because it offers many services, terminology changes, deprecated beta tools, paths that lead to the same location, interchange use of services and resources, apps and projects. In this post, I will try to clear things up based on my understanding. Feedback are welcome.

æˆ‘è™½ç„¶é˜…è¯»äº†ä¸€äº›å…³äºå¦‚ä½•IBM Cloudçš„æ–‡ç« ï¼Œä½†æ˜¯æˆ‘ä»ç„¶å­˜åœ¨ä¸€äº›ç–‘æƒ‘ã€‚ä¸»è¦æ˜¯å› ä¸ºå®ƒæä¾›äº†å¾ˆå¤šçš„æœåŠ¡ï¼Œæœ¯è¯­æ›´æ”¹ï¼Œè€å¥—çš„æµ‹è¯•å·¥å…·ï¼Œå¯¼è‡´ç›¸åŒçš„ä½ç½®çš„è·¯å¾„ï¼Œäº¤äº’ä½¿ç”¨æœåŠ¡å’Œèµ„æºï¼Œåº”ç”¨ç¨‹åºå’Œé¡¹ç›®ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­æˆ‘å°†æ ¹æ®æˆ‘çš„ç†è§£æ¥é˜è¿°è¿™äº›äº‹æƒ…ã€‚æ¬¢è¿åé¦ˆã€‚

Bluemix vs IBM Cloudâ€¨BluemixÂ is a cloud platform as a service which helps to build and run applications on the cloud. As of October 2017, Bluemix is IBM Cloud, although we still need to work with this domainÂ https://console.bluemix.net. IBM Cloud offers manyÂ products, for security, analytics, storage, AI,Â â€¦

Bluemix æ˜¯ä¸€ä¸ªäº‘æœåŠ¡å¹³å°ï¼ŒåŠ©åŠ›ä¸åœ¨äº‘ä¸Šæ„å»ºå’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚æˆªæ­¢äº2017å¹´10æœˆï¼ŒBluemix æ˜¯ IBM Cloudï¼Œå°½ç®¡æˆ‘ä»¬ä»éœ€è¦ä½¿ç”¨æ­¤åŸŸåhttps://console.bluemix.net.  IBM Cloudæä¾›äº†è®¸å¤šäº§å“ï¼Œç”¨äºå®‰å…¨ï¼Œæ•°æ®åˆ†æï¼Œæ•°æ®å­˜å‚¨ï¼ŒAIç­‰ç­‰ã€‚

>>>IBMâ€™s one-stop cloud computing shop provides all the cloud solutions 
>>>andÂ IBM cloudÂ tools you need

>>>IBMçš„ä¸€ç«™å¼äº‘è®¡ç®—å•†åº—æä¾›æ‚¨éœ€è¦çš„æ‰€æœ‰äº‘è§£å†³æ–¹æ¡ˆå’ŒIBMäº‘å·¥å…·ã€‚

Data Science and Watson Studio
æ•°æ®ç§‘å­¦ä¸Watson Studio

One of many products running on IBM Cloud isÂ IBM WatsonÂ for natural language processing, visual recognition and machine learning. We will useÂ Watson StudioÂ to â€œbuild, train, deploy and manage AI models, and prepare and analyze data, in a single, integrated environment.â€ It is initially calledÂ Data Science. Think of it like a front-end web page with lots of tools for interacting with Watson services. Letâ€™s get started!

è¿è¡Œåœ¨IBM Cloudä¸Šçš„è®¸å¤šäº§å“ä¹‹ä¸€æ˜¯ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œè§†è§‰è¯†åˆ«å’Œæœºæ¢°å­¦ä¹ çš„IBM Watson.æˆ‘ä»¬å°†ä½¿ç”¨Watson Studioåœ¨å•ä¸€çš„é›†æˆç¯å¢ƒä¸­æ„å»ºï¼Œè®­ç»ƒï¼Œéƒ¨ç½²å’Œç®¡ç†AIæ¨¡å‹ï¼Œå¹¶å‡†å¤‡å’Œåˆ†ææ•°æ®ã€‚å®ƒæœ€åˆè¢«ç§°ä¸ºæ•°æ®ç§‘å­¦ï¼Œå¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªå‰ç«¯ç½‘é¡µï¼Œå…¶ä¸­æœ‰è®¸å¤šä¸Watson servicesäº¤äº’çš„å·¥å…·ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚

>>>Watson Studio is a free workspace where you can seamlessly create, 
>>>evaluate, and manage your custom models.

>>>Watson Studio æ˜¯ä¸€ä¸ªå…è´¹çš„å·¥ä½œå®¤ï¼Œä½ å¯ä»¥æ— ç¼çš„åˆ›å»ºï¼Œè¯„ä¼°ä¸ç®¡ç†ä½ è‡ªå®šä¹‰çš„æ¨¡å‹

## Step 1: Sign up for IBMÂ Cloud

## æ³¨å†ŒIBMÂ Cloud

Go toÂ IBM CloudÂ and click on the buttonÂ Sign up for IBM Cloud.

è®¿é—®IBM Cloudç‚¹å‡»æ³¨å†ŒæŒ‰é’®è¿›è¡ŒIBM Cloudæ³¨å†Œ

After you successfully sign up, head over toÂ Watson StudioÂ and click Sign in.

æ³¨å†ŒæˆåŠŸä¹‹åï¼Œè¿”å›Watson Studioé¦–é¡µè¿›è¡Œç™»å½•

It will take you toÂ https://dataplatform.ibm.com/home. I think for historical reason, the domain name and the product name donâ€™t match ğŸ™„.

ç„¶åé¡µé¢å°†è·³è½¬åˆ° https://dataplatform.ibm.com/home.æˆ‘è®¤ä¸ºåº”è¯¥æ˜¯ä¹‹å‰çš„ä¸€äº›åŸå› ï¼Œå¯¼è‡´äº†åŸŸåå’Œäº§å“åç§°ä¸åŒ¹é…

## Step 2: Create new project for Visual Recognition

## åˆ›å»ºä¸€ä¸ªè§†è§‰è¯†åˆ«çš„æ–°å·¥ç¨‹

ClickÂ New projectÂ and select the toolÂ Visual Recognition, which is what we need to do classification on superheroes pictures.

ç‚¹å‡»æ–°å»ºå·¥ç¨‹å¹¶é€‰æ‹©Visual Recognitionå·¥å…·ï¼Œè¿™æ˜¯æˆ‘ä»¬éœ€è¦å¯¹è¶…çº§è‹±é›„å›¾ç‰‡åˆ†ç±»çš„å·¥å…·

>>>Quickly and accurately tag, classify and train visual content 
>>>using machine learning.

>>>ä½¿ç”¨æœºæ¢°å­¦ä¹ ï¼Œå¿«é€Ÿå‡†ç¡®åœ°å¯¹è§†è§‰å†…å®¹è¿›è¡Œæ ‡è®°ï¼Œåˆ†ç±»å’Œè®­ç»ƒ

A project is said to contain a set of tools and services. Letâ€™s name it Avengers. It comes with a storage.

ä¸€ä¸ªé¡¹ç›®åŒ…å«ä¸€å¥—å·¥å…·å’ŒæœåŠ¡ï¼Œè®©æˆ‘ä»¬å°†å®ƒå‘½åä¸ºå¤ä»‡è€…ã€‚å®ƒé…æœ‰ä¸€ä¸ªå­˜å‚¨

## Step 3: CreateÂ service

## åˆ›å»ºæœåŠ¡

For me, after creating new project, it goes to the pageÂ Default Custom Modeland says the project needs to associate with a service. I think the project should create a service and associate with it by default ğŸ™„

æˆ‘è®¤ä¸ºï¼Œåˆ›å»ºä¸€ä¸ªæ–°å·¥ç¨‹ä¹‹åï¼Œå°†ä¼šè·³è½¬åˆ°ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å¼çš„é¡µé¢ï¼Œè¯¥é¡¹ç›®éœ€è¦ä¸æœåŠ¡å…³è”ï¼Œæˆ‘è®¤ä¸ºé¡¹ç›®åº”è¯¥åˆ›å»ºä¸€ä¸ªæœåŠ¡å¹¶é»˜è®¤ä¸å…¶å…³è”

Go back to theÂ front page, select yourÂ AvengersÂ project fromÂ ProjectsÂ menu, and go toÂ SettingsÂ tab.

å›åˆ°ä¹‹å‰çš„å‰ç«¯é¡µé¢ï¼Œä»å·¥ç¨‹èœå•ä¸­é€‰æ‹©ä½ å¤ä»‡è€…é¡¹ç›®ï¼Œç„¶åè¿›å…¥è®¾ç½®é¡µé¢

Scroll down toÂ Associated servicesÂ section and click Watson. In the next screen we can see there are lots of services for many purposes: Text to Speech, Speech to Text, Language Translator, Tone Analyzer, etc. In our case, we needÂ Visual RecognitionÂ service. For Lite plan, we can have only 1 instance of each service, if you try to add more, you will get the following warning.

å‘ä¸‹æ»šåŠ¨åˆ°è”åˆæœåŠ¡åŒºå¹¶ç‚¹å‡»Watsoné€‰é¡¹ã€‚ä¸‹ä¸€å±æˆ‘ä»¬çœ‹åˆ°å¾ˆå¤šç”¨äºå¤šç§ç”¨é€”çš„æœåŠ¡ï¼Œæ–‡æœ¬è¯­è¨€ï¼Œè¯­è¨€æ–‡æœ¬ï¼Œè¯­è¨€ç¿»è¯‘å™¨ï¼ŒéŸ³é¢‘åˆ†æå™¨ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘éœ€è¦è§†è§‰è¯†åˆ«æœåŠ¡ã€‚å¯¹äºLiteè®¡åˆ’ï¼Œæ¯ä¸ªæœåŠ¡åªèƒ½æœ‰1ä¸ªå®ä¾‹ã€‚å¦‚æœä½ å°è¯•æ·»åŠ æ›´å¤šï¼Œåˆ™ä¼šæ”¶åˆ°ä»¥ä¸‹è­¦å‘Šï¼š

>>>Service broker error: You can only have one instance of a Lite plan 
>>>per service. To create a new instance, either delete your existing 
>>>Lite plan instance or select a paid plan.


The association between Watson projects, tools and services is confusing, probably because there are tons of features we havenâ€™t used yet. Think of project as a bag of tool and service. We can only use 1 instance of each service for Lite plan, and tool is the front end we use to interact with the service.

Watson projects å·¥å…·å’ŒæœåŠ¡ä¹‹å‰çš„å…³è”æ˜¯å¾ˆæ··ä¹±ï¼Œè‚¯èƒ½æ˜¯å› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰ä½¿ç”¨è¿‡å¾ˆå¤šåŠŸèƒ½ã€‚å°†é¡¹ç›®çœ‹åšä¸€ä¸ªå·¥å…·åŒ…å’ŒæœåŠ¡ã€‚æˆ‘ä»¬åªèƒ½ä¸ºLiteè®¡åˆ’ä½¿ç”¨æ¯ä¸ªæœåŠ¡çš„1ä¸ªå®ä¾‹ï¼Œè€Œå·¥å…·æ˜¯æˆ‘ä»¬ç”¨æ¥ä¸æœåŠ¡äº¤äº’çš„å‰ç«¯ã€‚

## Step 4: Remove existing service ifÂ any

## ç§»é™¤ç°æœ‰çš„æœåŠ¡ï¼ˆå¦‚æœ‰ï¼‰

To delete existing service, go back toÂ home page, selectÂ Watson ServicesÂ fromÂ ServicesÂ menu. Here we can launch the tool for this service or delete it.

è¦åˆ é™¤ç°æœ‰æœåŠ¡ï¼Œè¯·è¿”å›ä¸»é¡µï¼Œä»Servicesèœå•ä¸­é€‰æ‹©Watson Servicesã€‚ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å¯åŠ¨æ­¤æœåŠ¡çš„å·¥å…·æˆ–å°†å…¶åˆ é™¤ã€‚

After deleting existing service, go back to step 3 to configure new instance for serviceÂ Visual RecognitionÂ for this projectÂ Avengers.

åˆ é™¤ç°æœ‰çš„æœåŠ¡ã€‚è¿”å›åˆ°ç¬¬ä¸‰æ­¥ä¸ºé¡¹ç›®Avengersçš„è§†è§‰è¯†åˆ«æœåŠ¡é…ç½®ä¸€ä¸ªæ–°çš„å®ä¾‹

## Step 5: Create Visual recognition models

## åˆ›å»ºè§†è§‰è¯†åˆ«æ¨¡å‹

Go to yourÂ AvengersÂ project, clickÂ AssetsÂ tab and head over to sectionÂ Visual recognition models.Â For the fun part of this tutorial, I will name our custom modelÂ Avengers Models ğŸ¤˜.

è½¬åˆ°æ‚¨çš„å¤ä»‡è€…é¡¹ç›®ï¼Œç‚¹å‡»Assetsæ ‡ç­¾å¹¶è½¬åˆ°éƒ¨åˆ†è§†è§‰è¯†åˆ«æ¨¡å‹ã€‚ å¯¹äºæœ¬æ•™ç¨‹çš„æœ‰è¶£éƒ¨åˆ†ï¼Œæˆ‘å°†å‘½åæˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹å¤ä»‡è€…æ¨¡å‹ğŸ¤˜ã€‚

We can create class for each hero, or upload a zip file containing images for each hero. Note that the name of the zip file corresponds to the name of the class. The negative class is for images that do not fall into any expected classes. For this tutorial we will deal with Iron Man, Spider-Man, Captain America and Thor, because they are my favourites ğŸ˜….

æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸€ä¸ªè‹±é›„åˆ›å»ºä¸€ä¸ªç±»ï¼Œæˆ–è€…ä¸Šå»ä¸€ä¸ªåŒ…å«æ¯ä¸ªè‹±é›„çš„å›¾ç‰‡zipæ–‡ä»¶ã€‚æ³¨æ„ï¼Œæ¯ä¸ªzipæ–‡ä»¶çš„å‘½åå¿…é¡»å’Œåˆ›å»ºçš„ç±»çš„åå­—ç›¸å¯¹åº”ã€‚å¦å®šç±»æ˜¯é’ˆå¯¹ä¸å±äºä»»ä½•é¢„æœŸç±»åˆ«çš„å›¾åƒ

The more images we upload, the more correct the model is. Also, you should use more variations of the characters, in different angles, lights. Note that we should put correct images in each folder, because garbage in is garbage out

æˆ‘ä»¬ä¸Šä¼ çš„å›¾ç‰‡è¶Šå¤šï¼Œæ¨¡å‹è¶Šæ­£ç¡®ã€‚ æ­¤å¤–ï¼Œæ‚¨åº”è¯¥ä»¥ä¸åŒçš„è§’åº¦äº®ï¼Œä¸åŒçš„å…‰çº¿è§’åº¦ï¼Œä½¿ç”¨åŒä¸€ä¸ªè‹±é›„ã€‚ è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­æ”¾ç½®æ­£ç¡®çš„å›¾åƒï¼Œå› ä¸ºåƒåœ¾è¿›å…¥åƒåœ¾äº†

Select the buttonÂ Find and add imagesÂ on the top right to add images.

é€‰æ‹©å³ä¸Šè§’çš„æŸ¥æ‰¾å’Œæ·»åŠ å›¾åƒä»¥æ·»åŠ å›¾åƒã€‚

## Step 6: DataÂ set

## æ•°æ®é›†

We can download some free images from Google to use as our data set. You can get the data set on myÂ GitHub repo, or you can prepare the data set yourself. Downloading images manually is not fun, letâ€™s use a script. I donâ€™t know of any good tools, but a search from Google shows this toolÂ google-images-download. Weâ€™re lazy, letâ€™s save time.

æˆ‘ä»¬å¯ä»¥ä»è°·æ­Œä¸‹è½½ä¸€äº›å…è´¹çš„å›¾åƒä½œä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚ æ‚¨å¯ä»¥åœ¨æˆ‘çš„GitHubä»“åº“ä¸­è·å–æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥è‡ªå·±å‡†å¤‡æ•°æ®é›†ã€‚ æ‰‹åŠ¨ä¸‹è½½å›¾åƒå¹¶ä¸å¥½ç©ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è„šæœ¬ã€‚ æˆ‘ä¸çŸ¥é“ä»»ä½•å¥½çš„å·¥å…·ï¼Œä½†Googleçš„æœç´¢oogle-images-downloadæ˜¾ç¤ºè¿™ä¸ªå·¥å…·è°·æ­Œå›¾ç‰‡ä¸‹è½½ã€‚ æˆ‘ä»¬å¾ˆæ‡’ï¼Œè®©æˆ‘ä»¬èŠ‚çœæ—¶é—´ã€‚

50 images for each hero should be good in this post. Letâ€™s zip them and upload to Watson. After uploading finishes, add those assets to model.

è¿™è¾¹æ–‡ç« ä¸­ï¼Œæ¯ä¸ªè‹±é›„50å¼ å›¾ç‰‡åº”è¯¥å¯ä»¥è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœã€‚è®©æˆ‘ä»¬å‹ç¼©è¿™äº›å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°Watsonã€‚ä¸Šä¼ å®Œæˆä¹‹åã€‚æ·»åŠ è¿™äº›å›¾ç‰‡èµ„æºåˆ°æ¨¡å‹ä¸­ã€‚

## Step 7: Train theÂ model

## è®­ç»ƒæ¨¡å‹

This step requires all your remaining power to click thatÂ Train ModelÂ to start the training process. This takes some times depending on your dataset. For our dataset in this tutorial, it should take less than 10 minutes.

è¿™ä¸€æ­¥å°†éœ€è¦ä½ å…¨åŠ›ç‚¹å‡»è®­ç»ƒæ¨¡å‹æ¥å¼€å§‹è®­ç»ƒçš„è¿‡ç¨‹ã€‚è¿™ä¸ªè¿‡ç¨‹æ‰€æ¶ˆè€—çš„æ—¶é—´ä¾èµ–äºä½ çš„æ•°æ®é›†ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹çš„æ•°æ®é›†ï¼Œè®­ç»ƒè¿‡ç¨‹åº”è¯¥å°äº10åˆ†é’Ÿ

The reason it takes that short amount of time is because of our very small dataset. The other reason I think is because Visual Recognition uses a technique calledÂ transfer learning

è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹èŠ±è´¹çš„æ—¶é—´å¾ˆçŸ­æ˜¯ä¸€æ–¹é¢æ˜¯å› ä¸ºæ•°æ®é›†å¾ˆå°ï¼Œå¦ä¸€æ–¹é¢æˆ‘è®¤ä¸ºæ˜¯Visual Recognition ä½¿ç”¨äº†ä¸€ç§å«åštransfer learningçš„æŠ€æœ¯ã€‚

>>>Today, you can use â€œtransfer learningâ€â€Šâ€”â€Ši.e., use an existing image 
>>>recognition model and retrain it with your own dataset.

>>> ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨â€œtransfer learningâ€æŠ€æœ¯---å³ä½¿ç”¨ç°æœ‰çš„å›¾åƒè¯†åˆ«æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†é‡æ–°è®­ç»ƒã€‚

After training is complete, go toÂ ImplementationÂ tab then selectÂ Core MLÂ to download the CoreML compatible model. The file is 13MB.

è®­ç»ƒå®Œè¿™äº›æ¨¡å‹ä¹‹åï¼Œé¡µé¢è½¬åˆ°Implementationèœå•ï¼Œç„¶åé€‰æ‹©Core MLÂ é€‰é¡¹å»ä¸‹è½½å…¼å®¹CoreMLçš„æ¨¡å‹ï¼Œæ–‡ä»¶å¤§å°æœ‰13MB

## Using CoreML model in iOSÂ app

## åœ¨iOS APPä¸­ä½¿ç”¨CoreMLæ¨¡å‹

With the trained model from Watson, there are 2 ways to consume it in the app. The easy way is to use Watson SDK which wraps CoreML. The second way is to just use CoreML alone, and it is the approach we will show here, as it is simply to understand.

appä¸­ï¼Œæœ‰ä¸¤ç§æ–¹æ³•æ¥ä½¿ç”¨Watsonè®­ç»ƒå¥½çš„æ¨¡å‹ã€‚æœ€ç®€ç­”çš„æ–¹å¼ä½¿ç”¨åŒ…å«äº†CoreMLçš„Watson SDKã€‚ç¬¬äºŒç§æ–¹æ³•æ˜¯å•ç‹¬çš„ä½¿ç”¨CoreMLæ¡†æ¶ï¼Œè¿™æ˜¯æˆ‘ä»¬å°†è¦å±•ç¤ºçš„æ–¹æ³•ï¼Œå®ƒä»…ä»…æ˜¯ç®€å•çš„å¸®åŠ©æˆ‘ä»¬å»ç†è§£æ·±åº¦å­¦ä¹ 

Just to let you know that this SDK exists, we wonâ€™t use any framework in this tutorial ğŸ˜.

ä»…ä»…åªæ˜¯è®©ä½ çŸ¥é“æœ‰è¿™ç§SDKå­˜åœ¨ã€‚åœ¨è¿™è¾¹æ•™ç¨‹æˆ‘ä»¬ä¸ä½¿ç”¨ä»»ä½•çš„ç¬¬ä¸‰æ–¹æ¡†æ¶

	let classifierID = "your-classifier-id"

	let failure = { (error: Error) in print(error) }

	let image = UIImage(named: "your-image-filename")

	visualRecognition.classifyWithLocalModel(image: image, classifierIDs: 			[classifierID], failure: failure) {

					classifiedImages in print(classifiedImages)

	}

	let classifierID = "your-classifier-id"

	let failure = { (error: Error) in print(error) }

	visualRecognition.updateLocalModel(classifierID: classifierID, failure: failure) {

    print("model updated")

	}
	
## Use plain CoreMLÂ model

## ä½¿ç”¨çš„é€šç”¨çš„ CoreML æ¨¡å‹

In our guide, we just download the trained CoreML model and use it in our app. We learn the most when we donâ€™t use any extra unnecessary frameworks. The project is onÂ GitHub.

åœ¨è¿™ç¯‡æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»…ä»…ä¸‹è½½äº†è®­ç»ƒå¥½çš„CoreMLæ¨¡å‹ï¼Œåœ¨å†appä¸­ä½¿ç”¨å®ƒä»¬ã€‚å½“æˆ‘ä»¬ä¸éœ€è¦ä»»ä½•é¢å¤–çš„æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥å­¦åˆ°æ›´å¤šã€‚è¿™ä¸ªå·¥ç¨‹çš„[GitHubé“¾æ¥](https://github.com/onmyway133/Avengers)

### Running on the simulator

åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œ

The project usesÂ UIImagePickerControllerÂ withÂ controller.sourceType =Â .cameraÂ so itâ€™s great to build on device, take some pictures and predict. You can also run it on the simulator, just remember to pointÂ sourceTypeÂ toÂ photoLibraryÂ because thereâ€™s no camera in the simulator ğŸ˜….

é¡¹ç›®ä¸­ä½¿ç”¨äº†UIImagePickerControllerçš„controller.sourceType =  .cameraï¼Œæ‰€ä»¥æ˜¯å»ºç«‹åœ¨çœŸæœºè®¾å¤‡ä¸Šçš„ï¼Œæ‹æ‘„ç…§ç‰‡å’Œé¢„æµ‹æ˜¯å¾ˆæ£’çš„ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œï¼Œåªéœ€è¦è®°ä½å°†sourceTypeæ”¹ä¸ºphotoLibraryï¼Œå› ä¸ºæ¨¡æ‹Ÿå™¨ä¸Šæ²¡æœ‰ç›¸æœº

## Step 1: Add the model toÂ project

## æ·»åŠ æ¨¡å‹åˆ°é¡¹ç›®ä¸­

The model is what we use to make prediction. We just need to drag it to the project and add it to the app target. As reading fromÂ Integrating a Core ML Model into Your AppÂ we can just use the generated classÂ AvengersModels.

è¿™ä¸ªæ¨¡å‹å°±æ˜¯æˆ‘ä»¬ç”¨æ¥åšé¢„æµ‹çš„ä¸œè¥¿ã€‚ æˆ‘ä»¬åªéœ€å°†å…¶æ‹–åŠ¨åˆ°è¯¥é¡¹ç›®å¹¶å°†å…¶æ·»åŠ åˆ°åº”ç”¨ç¨‹åºç›®æ ‡ä¸­ã€‚ ä»é˜…è¯»[Integrating a Core ML Model into Your App](https://developer.apple.com/documentation/coreml/integrating_a_core_ml_model_into_your_app)ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç”Ÿæˆçš„ç±»AvengersModelsã€‚

>>>Xcode also uses information about the modelâ€™s inputs and outputs to 
>>>automatically generate a custom programmatic interface to the model, 
>>>which you use to interact with the model in your code.

>>>Xcodeè¿˜ä½¿ç”¨æœ‰å…³æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºçš„ä¿¡æ¯æ¥è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹çš„è‡ªå®šä¹‰ç¼–ç¨‹æ¥å£ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¯¥æ¨¡
>>>å‹ä¸ä»£ç ä¸­çš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚

## Step 2:Â Vision

## Vision
According toÂ wiki:

ä»¥ä¸‹æ˜¯ç»´åŸºç™¾ç§‘çš„å…³äºVisionçš„è§£é‡Š

>>>TheÂ VisionÂ is a fictionalÂ superheroÂ appearing inÂ American comic 
>>>booksÂ published byÂ Marvel Comics, anÂ androidÂ and a member of 
>>>theÂ AvengersÂ who first appeared inÂ The AvengersÂ #57 (October 1968)

>>>Visionæ˜¯ä¸€ä¸ªè™šæ„çš„è¶…çº§è‹±é›„ï¼Œå‡ºç°åœ¨æ¼«å¨æ¼«ç”»å‡ºç‰ˆçš„ç¾å›½æ¼«ç”»ä¹¦ä¸­ï¼Œæœºå™¨äººå’Œå¤ä»‡è€…çš„æˆ
>>>å‘˜æœ€åˆå‡ºç°åœ¨å¤ä»‡è€…è”ç›Ÿï¼ƒ57ï¼ˆ1968å¹´10æœˆï¼‰

Just kidding ğŸ˜…Â VisionÂ is a framework that works with CoreML â€œto apply classification models to images, and to preprocess those images to make machine learning tasks easier and more reliableâ€.

å¼€ç©ç¬‘ğŸ˜…ï¼ŒVisionæ˜¯ä¸€ä¸ªå¯ä¸CoreMLåä½œçš„æ¡†æ¶ï¼Œâ€œå°†åˆ†ç±»æ¨¡å‹åº”ç”¨äºå›¾åƒï¼Œå¹¶å¯¹è¿™äº›å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œä½¿æœºå™¨å­¦ä¹ ä»»åŠ¡å˜å¾—æ›´åŠ ç®€å•å’Œå¯é â€ã€‚

You should definitely watch WWDC 2017 videoÂ Vision Framework: Building on Core MLÂ to get to know some other features of this framework, like detecting faces, computing facial landmarks, tracking objects,Â â€¦

æ‚¨ç»å¯¹åº”è¯¥è§‚çœ‹WWDC 2017 WWDC è§†é¢‘Vision Frameworkï¼šåŸºäºCore MLæ„å»ºï¼Œä»¥äº†è§£æ­¤æ¡†æ¶çš„å…¶ä»–åŠŸèƒ½ï¼Œä¾‹å¦‚æ£€æµ‹é¢éƒ¨ï¼Œè®¡ç®—é¢éƒ¨æ ‡å¿—ï¼Œè·Ÿè¸ªå¯¹è±¡...

With the generatedÂ AvengersModels().modelÂ we can construct Vision compatible modelÂ VNCoreMLModelÂ and requestÂ VNCoreMLRequest, then finally send the request toÂ VNImageRequestHandler. The code is very straightforward:

ä½¿ç”¨ç”Ÿæˆçš„AvengersModels().modelï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºVisionå…¼å®¹æ¨¡å‹VNCoreMLModelå¹¶è¯·æ±‚VNCoreMLRequestï¼Œç„¶åå°†è¯·æ±‚å‘é€åˆ°VNImageRequestHandlerã€‚ ä»£ç éå¸¸ç®€å•ï¼š

	private func detect(image: UIImage) throws {

		loadingIndicator.startAnimating()
			
		let model = try VNCoreMLModel(for: AvengersModels().model)

		let request = VNCoreMLRequest(model: model, completionHandler: { [weak self] request, error in

	guard let results = request.results as? [VNClassificationObservation],

		let topResult = results.first else {
 
	   print(error as Any)

      return

    }

    DispatchQueue.main.async {

      self?.resultLabel.text = topResult.identifier + "(confidence \(topResult.confidence * 100)%)"

      self?.loadingIndicator.stopAnimating()

    }
    
	})

	 let handler = VNImageRequestHandler(cgImage: image.cgImage!, options: [:])

	DispatchQueue.global(qos: .userInteractive).async {

    do {

      try handler.perform([request])

    } catch {

      print(error)

	 	}
	 }
	}

The prediction operation may take time, so itâ€™s good habit to send it to background queue, and to update UI when we get the result.

é¢„æµ‹çš„æ“ä½œå¯èƒ½ä¼šæ¶ˆè€—ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥å°†è¿™ç§è€—æ—¶æ“ä½œæ”¾åœ¨åå°é˜Ÿåˆ—ä¸­æ˜¯ä¸ªå¥½ä¹ æƒ¯ï¼Œå½“å¾—åˆ°é¢„æµ‹çš„ç»“æœåæ›´æ–°UI

Build and run the app. If you build it on an iPhone, take a picture of a superhero, if that is one of the 4 superheroes we have trained in this tutorial, then CoreML will be able to predict who he is based on the trainedÂ .mlmodel

ç¼–è¯‘å¹¶è¿è¡ŒAppï¼Œå¦‚æœæ˜¯è¿è¡Œåœ¨iPhoneä¸Šï¼Œæ‹ä¸€å¼ è¶…çº§è‹±é›„çš„ç…§ç‰‡ï¼Œå¦‚æœæ˜¯æ‹çš„ç…§ç‰‡æ˜¯æˆ‘ä»¬æ•™ç¨‹ä¸­æ‰€è®­ç»ƒçš„4ä¸ªè¶…çº§è‹±é›„ä¸­çš„æŸä¸€ä¸ªï¼Œé‚£ä¹ˆCoreMLå°†ä¼šåŸºäºè®­ç»ƒå¥½çš„.mlmodelæ–‡ä»¶è¿›è¡Œé¢„æµ‹

# é™Œç”Ÿå•è¯

	dominant ä¸»è¦ï¼Œä¸»å¯¼ predicting é¢„æµ‹ consume æ¶ˆè€— ä½¿ç”¨
	straightforward ç›´æˆªäº†å½“ one-stop ä¸€ç«™å¼ seamlessly æ— ç¼çš„

