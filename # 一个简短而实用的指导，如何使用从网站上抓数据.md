[原文](https://towardsdatascience.com/a-short-practical-how-to-guide-to-scrape-data-from-a-website-using-python-888373227d4f)

# A short & practical HOW-TO guide to scrape data from a website using Python

# 一个简短而实用的指导，如何使用Python从网站上抓数据

_Note to the reader: Python code is shared at the end_

提示读者：Python 代码在文末有分享

This week I had to scrape a website for a client. I realized I did it so naturally and quickly that it would be useful to share it so you can master this art too._\[Disclaimer: this article shows my practices of scraping, if you have more relevant practices please share it in the comments\]_

这周我不得不为一个客户抓取一个网站的数据，我意识到我自然而然地做到了这一点，分享它会很有用，所以你也可以掌握这门艺术。_\[免责声明: 本文显示了我抓取数据的做法，如果你有更多的相关做法，请在评论中留言分享\]_

### The plan

### 计划

1. Pinpoint your target: a simple html website

   制定你的目标：一个简单的html 网站

2. Design your scraping scheme in Python

   使用Python设计抓取方案

3. Run & let the magic operate

   让运行Python代码运行起来



### Part I: Finding your target (a website)

### 第一部分：找到你的目标（一个网站）

In my case, I needed to gather the name of the Bank from SWIFT codes (or French BIC codes.) The website [http://bank-code.net/country/FRANCE-%28FR%29.html](http://bank-code.net/country/FRANCE-%28FR%29.html) has a list of 4000+ SWIFT codes with the associated bank names. The problem is they show only 15 results per page. Going through all the pages and copy paste 15 results at a time was NOT an option. Scraping came in handy for this task.

我的案例：我需要在SWIFT代码（或法国BIC代码）中收集银行的名称。[http://bank-code.net/country/FRANCE-%28FR%29.html](http://bank-code.net/country/FRANCE-%28FR%29.html) 有4000多个银行名称的SWIFT 代码。所面临的问题是该网站每页只显示15个代码，浏览所有的页面，然后每次复制粘贴15个结果，这样做并不可选。所以爬虫这时候就派上用场了

First, use Chrome “inspect” option to identify the part of html you need to get. Move your mouse on the different items in the inspection window (on the right), and track the part of website which is highlighted by the code (on the left). Once you’ve selected the item, in the inspection window, use “Copy / Copy element” and paste the html code in your python coding tool.

首先，使用 Chrome 的”检查“功能查看需要爬取的HTML部分。在检查窗口中（右边）在不同的item之间移动你的鼠标，网站部分会（左边）高亮在在代码追踪的时候。在检查窗口中，使用“Copy / Copy element”，并在python 编码工具中粘贴html代码

In my case, the desired item with 15 SWIFT codes is a “_table”_

在我的案例中：需要的15 个 SWIFT 代码的标签是一个 ”table“

### Part II: Design your scraping scheme in Python

### 第二部分：使用Python设计抓取方案

#### a) Scrape a first page

#### a)抓取第一个页面

```
import requests
url = "http://bank-code.net/country/FRANCE-%28FR%29/"
page = requests.get(url)
```

And that’s it, 3 lines of code and Python has received the webpage. Now you need to parse the html properly and retrieve the desired item.

就是这样，使用3行代码，Python就能获取到网页。现在，你需要解析HTML属性并检索所需要的标签

Remember the desired html :

记住需要的HTML

```
<table class="table table-hover table-bordered" id="tableID" style="margin-bottom: 10px;">
</table>
```

It is a “_table_” element, with id “_tableID_”. The fact that it has an id attribute is great, because no other html elements on this webpage can have this id. Which means if I look for this id in the html, I cannot find anything else than the desired element. It saves time.

这是一个id 为“”tableID“的table”元素。它具有id属性的事实很好，因为在改网页中没有其他的html元素有这个 值“tableID“的id了。意味着我在这个HTML中搜索tableID的元素，只会有一个元素。这就节约了时间。

Let’s do that properly in Python

使用Python实现这个功能

```

import bs4
soup = bs4.BeautifulSoup(page.content, 'lxml')
table = soup.find(name='table', attrs={'id':'tableID'})
```

So now we have got the desired html element. But we still need to get the SWIFT codes inside the html, and then store it in Python. I chose to store it in a pandas.DataFrame object, but just a list of list can work out as well.

现在我们获取到了想要的html元素，但是我们仍然需要在HTML中获取SWIFT代码，并使用Python存储。我选择将它存储在一个pandas.DataFrame的对象中，但是使用列表的列表也可以解决

To do that, go back on Chrome inspection window, analyse the structure of the html tree, and notice until which element you have to go. In my case, the required data was in “tbody” element. Each bank and its SWIFT code were contained in a “tr” element and each “tr” element had multiple “td” elements. The “td” elements contained the data I was looking for.

为此，返回到Chrome的检查窗口，分析html树型结构，并找到你需要的元素。在我的案例中，所需要的数据是在”tbody” 元素中，每个银行及其SWIFT代码包含在 ”tr“元素中，每个“tr”元素有多个“td”标签。“td”标签包含了我所搜索的数据



I do it in one line with the following:

使用下面的一样代码：

```
result = pd.DataFrame([[td.text for td in row.findAll('td')] for row in table.tbody.findAll('tr')])

```

#### b) Prepare automation

Now that we have scraped the first webpage, we need to think of how to scrape new webpages we haven’t seen yet. My way of doing that is replicating human behavior: storing results from one page, then going to the next. Let’s focus now on going to the next webpage.

At the bottom of the page, there is a menu that allows you to go on a specific page of the swift code table. Let’s inspect the “next page” button in the inspector window.

#### b)准备自动化

现在我们我么已经抓取奥第一页的数据了，我们应该考虑去抓取所看不到的新的网页。我这样做的方式复制人的行为：从一个存储结果，然后转到下一个页面，现在我们关注下一个网页

在页面的底部有一个菜单允许你去跳转到特定 SWIFT 代码列表的页面，在检查窗口中检查 “next page”。

This gives the following html element:

检查出以下的html元素

```
<a href="//bank-code.net/country/FRANCE-%28FR%29/15" data-ci-pagination-page="2" rel="next">&gt;</a>

```

Now to get the url in Python is simple:

获取a标签中的url使用如下Python代码

```
"http:" + soup.find('a', attrs={'rel':'next'}).get('href')

```

And we’re almost there.  
So far we have: 

 
\- developed the scraping of the table of one page 

 
\- identified the url link of the next page

到这里，我们大致已经做了：

- 抓取了一个页面的table标签找中的数据

- 确定了下一页的url链接

We only need to do a loop, and run the code. Two best practices I recommend following:

我们只需要做一个循环，然后运行代码，我推荐一下两个最好的练习：

1\. printing out when you land on a new webpage: to know at which stage of the process your code is (scraping codes can run for hours)

1.当加载新的页面时打印记录：为了知道你的代码每一个的流程

2\. saving results regularly: to avoid losing all you scraped if there is an error

2.定期保存结果：避免在发生错误时丢失你所抓取的数据

As long as I don’t know when to stop scraping, I loop with the idiomatic “_while True:_” syntax. I print out counter value at each step. And I save results in a csv file at each step as well. This can lose time actually, a better way would be to store data every 10 or 20 steps for instance. But I went for quick implementation.

只要我不知道何时停止抓取，我会循环使用惯用的“_while True:_”语法，我打印出每一步控台输出的值，并且我还将结果保存哎每个步骤中的csv文件中。由于我是为了快速的实现功能，所以这实际上可能会浪费时间，更好的方法是每10或20个步骤存储一次数据。



## 陌生单词

```
 retrieve 取回，检索
```


